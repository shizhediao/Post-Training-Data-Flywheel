# post-training-handbook

## Dataset

### Function Calling, Code, Math

| Name | Description | Domain | Quantity | Accuracy | Relevance |
|------|-------------|--------|----------|----------|-----------|
| [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) | One duplicate data in the first 10 entries | Function calling | 113K | 9/10 | 4.5 |
| [Salesforce/xlam-function-calling-60k](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k) | Answers are function names and parameter lists. Contains functions with ambiguous parameter types and trivial functions | Function calling | 60K | 10/10 | 4.5 |
| [Gorilla OpenFunctions-v2](https://github.com/ShishirPatil/gorilla/tree/main/data) | GitHub JSON format data, no Hugging Face dataset. Uses AST to determine if API calls are correct | Function calling | 65K | 10/10 | 5 |
| [ise-uiuc/Magicoder-OSS-Instruct-75K](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K) | Question 2 provides an answer, Question 3 does not provide partial implementation | Code | 75.2K | 9/10 | 3.5 |
| [RLHFlow/CodeUltraFeedback-standard](https://huggingface.co/datasets/RLHFlow/CodeUltraFeedback-standard) | RLH format, including chosen and rejected | Code | 38.4k | 8/10 | 4 |
| [codeparrot/apps](https://huggingface.co/datasets/codeparrot/apps) | | Code | 10K | | |
| [iamtarun/python_code_instructions_18k_alpaca](https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca) | | Code | 18.6K | | |
| [meta-math/MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA) | Question 4 does not provide public machine expressions | Math | 395k | 9.5/10 | 4.5 |
| [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct) | Contains 13 datasets, such as camel math, etc. Question 2 has no candidates, but the answer is correct. Question 3 does not provide a specific answer. Question 5 does not provide a specific answer. Most do not provide specific answers | Math | 262K | 9/10 | 3 |
| [camel-ai/math](https://huggingface.co/datasets/camel-ai/math) | | Math | 50k | 10/10 | 4.5 |
| [xinlai/Math-Step-DPO-10K](https://huggingface.co/datasets/xinlai/Math-Step-DPO-10K) | | Math | 10.8k | | |
| [openai/gsm8k](https://huggingface.co/datasets/openai/gsm8k) | Often used as a benchmark | Math | train 7.47k test 1.32K | | |



### Instruction Following

| Name | Description | Domain (Math/Coding/Instruct/Chat) | Number of Samples | Accuracy | Relevance | Note for Quality |
|------|-------------|-----------------------------------|-------------------|----------|-----------|------------------|
| [Open-Orca/1million-gpt-4](https://huggingface.co/datasets/Open-Orca/1million-gpt-4) | FLAN collection which has been augmented by submitting the listed question to GPT-4 | instruct | 1M | 5 | 4 | |
| [SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) | This release provides an efficient means of reacting our OpenOrca dataset with using larger slices of our data, while only including ~500k GPT-4 completions. | Instruct (1-turn chat) | 518k | 5 | 4 | |
| [GPT4-LLM](https://huggingface.co/datasets/teknium/GPT4-LLM-Cleaned) | Instruction-Following Data generated by GPT-4 using Alpaca prompts | Instruct | 54.6k | 5 | 4 | |
| [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) | Dolly2.0 (Pairs, English, 15K+ entries) — A dataset of human-written prompts and responses, featuring tasks like question-answering and summarization. | Instruct | 15k | 5 | 4 | |
| [allenai/WildChat (GPT4-EN)](https://huggingface.co/datasets/allenai/WildChat-1M) | 1 million conversations between human users and ChatGPT. 25.53% of the conversations come from the GPT-4 chatbot, while the rest come from the GPT-3.5 chatbot. | chat, instruct | 168k | 4 | 5 | filter gpt-4-en |
| [sablo/oasst2_curated](https://huggingface.co/datasets/sablo/oasst2_curated) | A filtered and curated dataset taken from the top scoring OpenAssistant/oasst2 conversations. Saved in HF Chat format. The result is a high quality dataset for SFT. | chat | train 4.69k, test 24 | 5 | 4 | open-ended conversation, human annotated |
| [CollectiveCognition/chats-data-2023-09-22](https://huggingface.co/datasets/CollectiveCognition/chats-data-2023-09-22) | Collection of chats between users and the ChatGPT model. These conversations have been shared by users on the "Collective Cognition" website. | chat, instruct | 156 | 4.75 | 4 | Human: after filter out GPT-4 |
| [lmsys/lmsys-chat-1m](https://huggingface.co/datasets/lmsys/lmsys-chat-1m) | one million real-world conversations with 25 state-of-the-art LLMs. | chat, instruct | 1M | 4.5 | 4 | Human: after filter out GPT-4 |
| [GPTeacher](https://huggingface.co/datasets/teknium/GPTeacher-General-Instruct) | GPT-4 Generated self-instruct dataset. | Instruct | 89.3k | 4.5 | 4 | gpt-4 generated |
| [UltraChat](https://hf.co/datasets/stingning/ultrachat) | Some data inside the 774K are very long, basically exceeding 10000 in length | Chat | 774k | 4.5 | 4 | Human: The dialogue is a list of strings chatgpt generated with human refinements |
| [jondurbin/airoboros-3.2](https://huggingface.co/datasets/jondurbin/airoboros-3.27not-for-all-audiences=true) | modified self-instruct gpt4 | instruct | 58,709 | 4.5 | 4 | Accuracy: Errors in mathematical calculations. Data was generated primarily with gpt-4 |
| [UltraInteract](https://huggingface.co/datasets/openbmb/UltraInteract_sft) | a large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. | Instruct, with coding, math, etc. as sub-tasks | 289K | 4 | 5 | specifically for reasoning |
| [AutoIF](https://github.com/QwenLM/AutoIF) | Synthetic dataset that matches IFEval, no open source download available | Instruct | N/A | N/A | N/A | hack IFEval to generate data |
| [WizardLM_evol_instruct_V2_196k](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k) | original wizard lm data | Instruct | 286k | 4.5 | 3 | Human: Some errors gpt-3.5-turbo generated |
| TIGER-Lab/WebInstructSub | vast amounts of high-quality instruction data exist in the web corpus, spanning various domains like math and science. | instruct (math, science Q&A) | 2.34M | 5 | 3 | Human: not relevant |
| [soda](https://huggingface.co/datasets/allenai/soda) | Dialogue dataset covering a wide range of social interactions. | Chat | train 1.19M validation 146k test 149k | 5 | 3 | Accuracy: Discrepancy in the amount of dialogue and conversation data. Dialogue contains proper_name information. Human: not GPT-4 level |
| [Daring-Anteater](https://huggingface.co/datasets/nvidia/Daring-Anteater) | consisting of 100k conversations, each averaging 2.88 model turns, generated using NVIDIA proprietary model and Mistral-8x7B-Instruct-v0.1, while the remaining samples are sourced from FinQA, wikitablequestions, and commercially-friendly subsets of Open-Platypus | Chat | 99.5k | 5 | 3 | Human: from NVIDIA proprietary models and Mistral-8x7B-Instruct-v0.1 not GPT-4 |
| [AlpacaDataCleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned) | Some Alpaca/ LLaMA-like models (Pairs, English) — Cleaned version of Alpaca, GPT_LLM, and GPTeacher. | Instruct | 52k | 5 | 3 | |
| [Alpaca data](https://huggingface.co/datasets/tatsu-lab/alpaca) | ChatGLM-fine-tune-LoRA; Koala (Dialog, Pairs, English, 52K entries, 21.4MB) — A dataset generated by text-davinci-003 to enhance language models' ability to follow human instruction. | Instruct | 52k | 4.5 | 3 | |
| [ChatAlpaca](https://github.com/cascip/ChatAlpaca) | use ChatGPT (GPT-3.5-turbo) to generate follow-up utterances and continue the conversation with ChatGPT | chat, instruct | 20k | 4 | 3 | |
| [ShareGPT](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) | | Chat | 9.03k rows | 3 | 3 | Accuracy: Many foreign languages. Human: After filtering, a high-quality GPT4 daily Q&A dataset, size 6K, mainly knowledge Q&A, programming questions, reasoning calculations, including Simplified Chinese, Traditional Chinese, English, Japanese, Korean, and various languages |
| [GPT-4all](https://huggingface.co/datasets/andersonbcdefg/gpt4all) | Questions from stackoverflow | 1-turn chat, user-assistant interaction | 438k | 3 | 2 | Human: prompt is html coding and math, not relevant to instruction following |
| [Open Assistant 2](https://huggingface.co/datasets/sablo/oasst2_curated https://huggingface.co/datasets/OpenAssistant/ https://huggingface.co/OpenAssistant https://huggingface.co/datasets/OpenAssistant/oasst1 https://huggingface.co/datasets/OpenAssistant/oasst2) | oasst2 with 56k conversations | 1. Chat 2.1 Chat 2.2 Chat 2.3 All language versions 2.4 Chat | 1. train 4.69k, test 247 2.1 train 84.4k, validation 4.4k 2.2 train 12.9k, validation 690 2.3 3.72k 2.4 train 129k, validation 6.6k | 2 | 1 | Accuracy: 1: 4 2.1: ? |
| [OpenAssistant/ oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1) | This version of the dataset contains data collected on the open-assistant.io website until April 12 2023. | chat | train:84.4k val:4.4k | N/A | N/A (4) | need process conversation tree to inspect data |
| [OpenAssistant/ oasst2](https://huggingface.co/datasets/OpenAssistant/oasst2) | This version of the dataset contains data collected on the open-assistant.io website until Nov 5 2023. | chat | train:129k val:6.6k | N/A | N/A (4) | need process conversation tree to inspect data |
| [Salesforce/dialogstudio](https://huggingface.co/datasets/Salesforce/dialogstudio) | Towards Richest and Most Diverse Unified Dataset Collection and Instruction-Aware Models for Conversational AI | Chat ? | See details at https://github.com/salesforce/DialogStudio/blob/main/Dataset_Statistics.csv train: 600 ? val: 200 ? test: 400 ? | 0 | 2 | focus on conversational AI, irrelevant Accuracy: Cannot be used online, need to download locally first. Organize dialogues in the form of hum list, including many other auxiliary information |
| argilla/magpie-ultra-v0.1 | synthetically generated dataset for supervised fine-tuning using the new Llama 3.1 (70B-turbo) model together with other Llama models like Llama-Guard-3-5B and Meta-Llama-3.1-8B-Instruct. | instruction | 50k | 4.75 | 3.5? | llama-3.1-40B generated |
| [bigscience/P3](https://huggingface.co/datasets/bigscience/P3) |  | Instruct (QA) | 122,039,002 1220396 (?) Directly obtained from huggingface num rows | 5 | 3 | Responses are short, mostly 1-2 sentences |
| [yizhongw/self_instruct](https://huggingface.co/datasets/yizhongw/self_instruct) | The huggingface dataset also includes P3 and Super Natural Instructions data. Self-Instruct is a framework that helps language models improve their ability to follow natural language instructions. It does this by using the model's own generations to create a large collection of instructional data. With Self-Instruct, it is possible to improve the instruction-following capabilities of language models without relying on extensive manual annotation. | Instruct | 82.6k |  | 3 | Human: not GPT-4 level |
| [meta-llama/Meta-Llama-3.1-8B-Instruct-evals](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals) | This dataset contains the Meta evaluation result details for Meta-Llama-3.1-8B-Instruct. The dataset has been created from 30 evaluation tasks. | Instruct, function calling | 157,823 157k (?) |  | 3 | Human: not GPT-4 level, llama3 generated on benchmarks! |
| [mosaicml/instruct-v3](https://huggingface.co/datasets/mosaicml/instruct-v3) | Each piece of data has a marked source. This is an aggregate dataset comprised of Dolly, HFRLHF (derived from Databricks Dolly) Self-Instruct (Yizhong Wang) and HH (Anthropic Harmless) datasets, combined with Competition Math, Duorc, CoT GSM8k, Qasper, Quality, Summ Screen FD and Spider. | Instruct | train 56.2k test 6.81k |  | 2 | not GPT-4 level, irrelevant task |
| [OpenHermes2.5](https://hf.co/datasets/teknium/OpenHermes-2.5) | Airoboros 2.2 + CamelAI Domain Expert Datasets (Physics, Math, Chemistry & Biology) + Fatidici4K-orca CoT + GPT4 Collective Cognition (09-10-2023 ~ CoT) + Alpaca GPT4 + Evol Instruct 70K && 140K + Glaive Code Assistant + GPT4-LLM + GPTeacher + Medical Tasks + MetaMath 40k + SlimOrca 550K + Platypus + ShareGPT (GPT4-Only) + Unnatural Instructions GPT4 | Chat instruct, including coding, math, etc. | 1M |  |  | naive mixture of multiple datasets Filtering included removal of OpenAI refusals, disclaimers, and "As an AI" type examples and more |
| [bilexi/Bitext-customer-support-llm-chatbot-training-dataset](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset) | The user provides questions, and the response is a prompt from the assistant | Instruct | 26.9k |  | 2 | irrelevant (Customer Service) |

# Quality Check
- Domain: we are only concerned about the following tasks: instruction following, coding, math. Datasets other than those in English are not considered.
- Data source: only keep GPT-4 generated data. Drop inferior data sources (gpt-3.5-turbo).
- popular dataset, download > 1K.
- Accuracy (%): randomly sample 20 for instruction tuning dataset and 10 for other domains. Check the quality manually and provide quality signal = x / 20
- Relevance Score (1-5):
    - 5: Directly corresponds to one of [IFEval*, MTBench, AGIEval*, AlpacaEval, …] (Overfitting)
    - 4: Generally have instruction following format and GPT-4 / human level response.
    - 3: Most have instruction following format and correct response.
    - 2: Have major flaws (e.g. irrelevant) but maybe useful
    - 1: low quality or potential harmful impact
